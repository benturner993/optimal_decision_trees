{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "from scipy import stats\n",
    "plt.rc(\"figure\", figsize=(160,80))\n",
    "plt.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretableai import iai\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df=pd.read_csv('/Users/Ben/Desktop/optimal_decision_trees/data/modelling.csv')\n",
    "kaggle_df=pd.read_csv('/Users/Ben/Desktop/optimal_decision_trees/data/test.csv')\n",
    "holdout_df=pd.read_csv('/Users/Ben/Desktop/optimal_decision_trees/data/holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_categoric_columns(df):\n",
    "    \n",
    "    ''' function to return categoric columns '''\n",
    "\n",
    "    all_columns = list(df.columns)\n",
    "    numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\n",
    "    numeric_columns = df.select_dtypes(include=numeric_types).columns.to_list()\n",
    "    categoric_columns = list(set(all_columns) - set(numeric_columns))\n",
    "    return categoric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_bin_cols(df, threshold):\n",
    "    \n",
    "    ''' function to group up values with low frequency based on a specified threshold'''\n",
    "\n",
    "    # empty lists\n",
    "    fe_cols=[]\n",
    "    fe_transform=[]\n",
    "\n",
    "    for i in categoric_columns:\n",
    "\n",
    "        if len(df[i].unique())>=8:\n",
    "\n",
    "            fe_cols.append(i)\n",
    "            fe_transform.append(df[i].map(df.groupby(i)[i].size().sort_values(ascending=False).div(len(df)).cumsum().le(threshold)))\n",
    "\n",
    "    for u, v in zip(fe_cols, fe_transform):\n",
    "        \n",
    "        df[u] = np.where(v, df[u], 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_transformer(df1, df2, col):\n",
    "\n",
    "    ''' function to transform datasets to match '''\n",
    "    \n",
    "    list1=list(sorted(df1[col].unique()))\n",
    "    list2=list(sorted(df2[col].unique()))\n",
    "\n",
    "    intersection=list(set(list1).intersection(list2))\n",
    "\n",
    "    df2[col]=np.where(df2[col].isin(intersection),df2[col],'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_cols(df1, df2):\n",
    "    \n",
    "    ''' function to compare the levels in two dataframes and return the columns which align '''\n",
    "\n",
    "    # find categoric columns\n",
    "    all_columns = list(df2.columns)\n",
    "    numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\n",
    "    numeric_columns = df2.select_dtypes(include=numeric_types).columns.to_list()\n",
    "    categoric_columns = list(set(all_columns) - set(numeric_columns))\n",
    "\n",
    "    # create empty list for columns and indicators\n",
    "    cols=[]\n",
    "    cols_ind=[]\n",
    "\n",
    "    # loop through all categoric columns and create indicators\n",
    "    for i in categoric_columns:\n",
    "        df_t_unique=sorted(df1[i].unique())\n",
    "        df_te_unique=sorted(df2[i].unique())\n",
    "        cols.append(i)\n",
    "        cols_ind.append(df_t_unique==df_te_unique)\n",
    "\n",
    "    # filter for relevant columns\n",
    "    d = {'cols':cols,'cols_ind':cols_ind}\n",
    "    df = pd.DataFrame(d)\n",
    "    cat_cols=df.loc[df['cols_ind']==True]['cols']\n",
    "\n",
    "    # combine with numerics and add loss\n",
    "    modelling_cols = numeric_columns + sorted(list(cat_cols))\n",
    "    modelling_cols.append('loss')\n",
    "\n",
    "    return modelling_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring_cols=scoring_cols(training_df, kaggle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_validation_subset(df):\n",
    "    ''' function to create training and validation subsets\n",
    "        chosen this methodology as a method to replicate in the future '''\n",
    "\n",
    "    training_df = df.sample(frac=0.7)\n",
    "    print('Modelling dataset rows:\\t', training_df.shape[0])\n",
    "\n",
    "    validation_df = pd.concat([df, training_df]).drop_duplicates(keep=False)\n",
    "    print('Validation dataset rows:\\t', validation_df.shape[0])\n",
    "\n",
    "    return training_df, validation_df\n",
    "\n",
    "# train_df=training_df.loc[training_df.index<=160000]\n",
    "# modelling_df, validation_df=training_validation_subset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_encoding(df):\n",
    "\n",
    "    # find all relevant columns\n",
    "    all_columns = list(df.columns)\n",
    "    numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']\n",
    "    numeric_columns = df.select_dtypes(include=numeric_types).columns.to_list()\n",
    "    categoric_columns = list(set(all_columns) - set(numeric_columns))\n",
    "\n",
    "    for i in categoric_columns:\n",
    "        df[i] = df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group up categoric values with many levels\n",
    "# look for cols with many levels\n",
    "# group up based on response \n",
    "# segment into n levels / groups\n",
    "\n",
    "# or frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_outlier_capping(df, variable, multiplier):\n",
    "\n",
    "    ''' windsorise the response variable '''\n",
    "\n",
    "    q1 = np.percentile(df[variable],25)\n",
    "    q3 = np.percentile(df[variable],75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (iqr * multiplier)\n",
    "    upper = q3 + (iqr * multiplier)\n",
    "\n",
    "    df[variable] = np.where(df[variable]<=lower, lower, df[variable])\n",
    "    df[variable] = np.where(df[variable]>=upper, upper, df[variable])\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_response(df, response):\n",
    "\n",
    "    ''' take the natural log of the response variable '''\n",
    "\n",
    "    print('Skewness of untransformed response:\\t' + str(df[response].skew()))\n",
    "\n",
    "    # transform response column to ensure +ve\n",
    "    minimum_val = math.ceil(min(abs(np.log(df[response]))))\n",
    "    original_data = np.log(df[response]) + minimum_val\n",
    "    df[response] = np.log(df[response])\n",
    "    print('Skewness of transformed response:\\t' + str(df[response].skew()))\n",
    "\n",
    "    return df\n",
    "\n",
    "training_df=response_outlier_capping(training_df, 'loss', 2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_columns=return_categoric_columns(training_df)\n",
    "frequency_bin_cols(training_df, 0.8)\n",
    "categorical_encoding(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a GridSearch to fit an OptimalTreeRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(training_df.columns)\n",
    "X = training_df.loc[:, ~training_df.columns.isin(['loss', 'id'])]\n",
    "y = np.log(training_df['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = iai.split_data('regression', X, y,\n",
    "                                                      train_proportion=0.90,\n",
    "                                                      seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to choose a starting value for regression_lambda. You can either use the default value, or find a good starting estimate yourself.\n",
    "# One approach to doing this yourself cheaply is to validate over regression_lambda with max_depth fixed to zero - this is effectively just fitting a linear regression to the data and allows you to find a good baseline level of regularization:\n",
    "\n",
    "# grid = iai.GridSearch(\n",
    "#     iai.OptimalTreeRegressor(\n",
    "#         random_seed=2,\n",
    "#         max_depth=0,\n",
    "#         #regression_sparsity=:all,\n",
    "#     ),\n",
    "#     regression_lambda=[0.0001, 0.001, 0.01, 0.1],\n",
    "# )\n",
    "# iai.fit!(grid, X, y)\n",
    "# starting_lambda = IAI.get_best_params(grid)[:regression_lambda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the starting estimate from Step 1 for regression_lambda, we now tune max_depth:\n",
    "\n",
    "# grid = IAI.GridSearch(\n",
    "#     IAI.OptimalTreeRegressor(\n",
    "#         random_seed=1,\n",
    "#         regression_sparsity=:all,\n",
    "#         regression_lambda=starting_lambda,\n",
    "#     ),\n",
    "#     max_depth=1:3,\n",
    "# )\n",
    "# IAI.fit!(grid, X, y)\n",
    "# best_depth = IAI.get_best_params(grid)[:max_depth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we fix max_depth to the value found in Step 2, and tune regression_lambda to get the final result:\n",
    "\n",
    "# grid = IAI.GridSearch(\n",
    "#     IAI.OptimalTreeRegressor(\n",
    "#         random_seed=1,\n",
    "#         max_depth=best_depth,\n",
    "#         regression_sparsity=:all,\n",
    "#     ),\n",
    "#     regression_lambda=[0.0001, 0.001, 0.01, 0.1],\n",
    "# )\n",
    "# IAI.fit!(grid, X, y)\n",
    "# IAI.get_best_params(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdepth=9\n",
    "grid = iai.GridSearch(\n",
    "    iai.OptimalTreeRegressor(\n",
    "        random_seed=123,\n",
    "    ),\n",
    "    max_depth=maxdepth,\n",
    ")\n",
    "grid.fit(train_X, train_y) # https://docs.interpretable.ai/stable/OptimalTrees/tuning/\n",
    "grid.get_learner()\n",
    "\n",
    "# range(9, maxdepth),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions on new data using predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Result\n",
    "y_pred=np.exp(grid.predict(train_X))\n",
    "result=mean_absolute_error(y_pred, train_y)\n",
    "print('train result: ', result)\n",
    "\n",
    "# Test Result\n",
    "y_pred=np.exp(grid.predict(test_X))\n",
    "result=mean_absolute_error(y_pred, test_y)\n",
    "print('test result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_cats=return_categoric_columns(holdout_df)\n",
    "\n",
    "for i in holdout_cats:\n",
    "    frequency_transformer(training_df, holdout_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout df\n",
    "X_test= holdout_df[cols].loc[:, ~holdout_df.columns.isin(['loss', 'id'])]\n",
    "\n",
    "# Encode\n",
    "categorical_encoding(X_test)\n",
    "\n",
    "grid.predict(X_test)\n",
    "\n",
    "y_pred=np.exp(grid.predict(X_test))\n",
    "y_valid=holdout_df['loss']\n",
    "result=mean_absolute_error(y_pred, y_valid)\n",
    "print('result: ', result)\n",
    "\n",
    "idx=holdout_df['id']\n",
    "d = {'id':idx,'loss':y_pred}\n",
    "out_df=pd.DataFrame(d)\n",
    "out_df.to_csv(f'/Users/Ben/Desktop/optimal_decision_trees/outputs/holdout_odt_predictions_maxdepth{maxdepth}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_cats=return_categoric_columns(kaggle_df)\n",
    "\n",
    "for i in holdout_cats:\n",
    "    frequency_transformer(training_df, kaggle_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle df\n",
    "X_kaggle= kaggle_df.loc[:, ~kaggle_df.columns.isin(['id'])]\n",
    "\n",
    "# Encode\n",
    "categorical_encoding(X_kaggle)\n",
    "\n",
    "idx=kaggle_df['id']\n",
    "y_pred=np.exp(grid.predict(X_kaggle))\n",
    "d = {'id':idx,'loss':y_pred}\n",
    "out_df=pd.DataFrame(d)\n",
    "out_df.to_csv(f'/Users/Ben/Desktop/optimal_decision_trees/outputs/kaggle_odt_predictions_maxdepth{maxdepth}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.write_html(f'/Users/Ben/Desktop/optimal_decision_trees/outputs/odt_model_maxdepth{maxdepth}.html')\n",
    "grid.write_json(f'/Users/Ben/Desktop/optimal_decision_trees/outputs/odt_model_maxdepth{maxdepth}.json')\n",
    "#lnr = IAI.read_json(\"learner.json\") https://docs.interpretable.ai/stable/IAIBase/learner/#Parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
